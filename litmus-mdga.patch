diff -Naur litmus-rt-original/include/litmus/fdso.h litmus-rt-mdga/include/litmus/fdso.h
--- litmus-rt-original/include/litmus/fdso.h	2017-07-12 22:57:41.000000000 +0200
+++ litmus-rt-mdga/include/litmus/fdso.h	2019-05-17 13:58:42.704577484 +0200
@@ -26,8 +26,8 @@
 	PCP_SEM         = 5,
 
 	DFLP_SEM	= 6,
-
-	MAX_OBJ_TYPE	= 6
+    MDGA_SEM    =7,
+	MAX_OBJ_TYPE	= 7
 } obj_type_t;
 
 struct inode_obj_id {
diff -Naur litmus-rt-original/include/litmus/litmus.h litmus-rt-mdga/include/litmus/litmus.h
--- litmus-rt-original/include/litmus/litmus.h	2017-07-12 22:57:41.000000000 +0200
+++ litmus-rt-mdga/include/litmus/litmus.h	2019-05-20 17:32:14.115499234 +0200
@@ -27,6 +27,7 @@
 }
 
 struct task_struct* __waitqueue_remove_first(wait_queue_head_t *wq);
+struct task_struct* __check_waitqueue_first(wait_queue_head_t *wq);
 
 #define NO_CPU			0xffffffff
 
@@ -70,6 +71,15 @@
 #define get_rt_phase(t)		(tsk_rt(t)->task_params.phase)
 #define get_partition(t) 	(tsk_rt(t)->task_params.cpu)
 #define get_priority(t) 	(tsk_rt(t)->task_params.priority)
+
+/* new macros added to support MDGA */
+#define get_jobno(t)		(tsk_rt(t)->job_params.job_no)
+#define get_total_jobs(t)	(tsk_rt(t)->task_params.total_jobs)
+#define get_rt_order(t)		(tsk_rt(t)->task_params.job_order)
+#define get_cs_total(t)		(tsk_rt(t)->task_params.total_cs)
+#define get_current_cs(t)	(tsk_rt(t)->task_params.current_cs)
+#define get_rt_ddls(t)      (tsk_rt(t)->task_params.relative_ddls)
+
 #define get_class(t)        (tsk_rt(t)->task_params.cls)
 #define get_release_policy(t) (tsk_rt(t)->task_params.release_policy)
 
diff -Naur litmus-rt-original/include/litmus/rt_param.h litmus-rt-mdga/include/litmus/rt_param.h
--- litmus-rt-original/include/litmus/rt_param.h	2017-07-12 22:57:41.000000000 +0200
+++ litmus-rt-mdga/include/litmus/rt_param.h	2019-05-20 17:32:14.135499245 +0200
@@ -122,6 +122,16 @@
 	task_class_t	cls;
 	budget_policy_t  budget_policy;  /* ignored by pfair */
 	release_policy_t release_policy;
+    /* the jobs' order within one task */
+    unsigned int	job_order[32];
+    /* the number of jobs within one hyper period */
+    unsigned int	total_jobs;
+    /* the total number of jobs within one hyper period */
+    unsigned int	total_cs;
+    /* current serving critical section */
+    unsigned int    current_cs;
+    /* define the deadlines of all the sub-jobs */
+    lt_t            relative_ddls[32];
 };
 
 /* don't export internal data structures to user space (liblitmus) */
diff -Naur litmus-rt-original/include/litmus/wait.h litmus-rt-mdga/include/litmus/wait.h
--- litmus-rt-original/include/litmus/wait.h	2017-07-12 22:57:41.000000000 +0200
+++ litmus-rt-mdga/include/litmus/wait.h	2019-05-17 15:40:49.092070574 +0200
@@ -2,6 +2,7 @@
 #define _LITMUS_WAIT_H_
 
 struct task_struct* __waitqueue_remove_first(wait_queue_head_t *wq);
+struct task_struct* __check_waitqueue_first(wait_queue_head_t *wq);
 
 /* wrap regular wait_queue_t head */
 struct __prio_wait_queue {
diff -Naur litmus-rt-original/litmus/fdso.c litmus-rt-mdga/litmus/fdso.c
--- litmus-rt-original/litmus/fdso.c	2017-07-12 22:57:41.000000000 +0200
+++ litmus-rt-mdga/litmus/fdso.c	2019-05-17 15:41:28.416092994 +0200
@@ -28,6 +28,7 @@
 	&generic_lock_ops, /* DPCP_SEM */
 	&generic_lock_ops, /* PCP_SEM */
 	&generic_lock_ops, /* DFLP_SEM */
+    &generic_lock_ops, /* MDGA_SEM */
 };
 
 static int fdso_create(void** obj_ref, obj_type_t type, void* __user config)
diff -Naur litmus-rt-original/litmus/jobs.c litmus-rt-mdga/litmus/jobs.c
--- litmus-rt-original/litmus/jobs.c	2017-07-12 22:57:41.000000000 +0200
+++ litmus-rt-mdga/litmus/jobs.c	2019-05-26 17:11:33.921015765 +0200
@@ -12,13 +12,20 @@
 
 static inline void setup_release(struct task_struct *t, lt_t release)
 {
-	/* prepare next release */
+    /* update job sequence number */
+    t->rt_param.job_params.job_no++;
+
+    /* prepare next release */
 	t->rt_param.job_params.release = release;
-	t->rt_param.job_params.deadline = release + get_rt_relative_deadline(t);
+	if (t->rt_param.job_params.job_no > 2) {
+        t->rt_param.job_params.deadline = release + get_rt_ddls(t)[(t->rt_param.job_params.job_no - 3)%(t->rt_param.task_params.total_jobs)];
+
+	} else {
+        t->rt_param.job_params.deadline = release + get_rt_relative_deadline(t);
+    }
 	t->rt_param.job_params.exec_time = 0;
 
-	/* update job sequence number */
-	t->rt_param.job_params.job_no++;
+
 
 	/* expose to user space */
 	if (has_control_page(t)) {
diff -Naur litmus-rt-original/litmus/litmus.c litmus-rt-mdga/litmus/litmus.c
--- litmus-rt-original/litmus/litmus.c	2017-07-12 22:57:41.000000000 +0200
+++ litmus-rt-mdga/litmus/litmus.c	2019-05-20 11:32:27.115190951 +0200
@@ -129,12 +129,12 @@
 		goto out_unlock;
 	if (tp.period <= 0)
 		goto out_unlock;
-	if (min(tp.relative_deadline, tp.period) < tp.exec_cost) /*density check*/
-	{
-		printk(KERN_INFO "litmus: real-time task %d rejected "
-		       "because task density > 1.0\n", pid);
-		goto out_unlock;
-	}
+	//if (min(tp.relative_deadline, tp.period) < tp.exec_cost) /*density check*/
+	//{
+	//	printk(KERN_INFO "litmus: real-time task %d rejected "
+	//	       "because task density > 1.0\n", pid);
+	//	goto out_unlock;
+	//}
 	if (tp.cls != RT_CLASS_HARD &&
 	    tp.cls != RT_CLASS_SOFT &&
 	    tp.cls != RT_CLASS_BEST_EFFORT)
diff -Naur litmus-rt-original/litmus/locking.c litmus-rt-mdga/litmus/locking.c
--- litmus-rt-original/litmus/locking.c	2017-07-12 22:57:41.000000000 +0200
+++ litmus-rt-mdga/litmus/locking.c	2019-05-17 15:46:22.744260812 +0200
@@ -139,6 +139,18 @@
 	return(t);
 }
 
+struct task_struct* __check_waitqueue_first(wait_queue_head_t *wq)
+{
+	wait_queue_t* q;
+	struct task_struct* t = NULL;
+
+	if (waitqueue_active(wq)) {
+		q = list_entry(wq->task_list.next, wait_queue_t, task_list);
+		t = (struct task_struct*) q->private;
+	}
+	return(t);
+}
+
 unsigned int __add_wait_queue_prio_exclusive(
 	wait_queue_head_t* head,
 	prio_wait_queue_t *new)
diff -Naur litmus-rt-original/litmus/sched_gsn_edf.c litmus-rt-mdga/litmus/sched_gsn_edf.c
--- litmus-rt-original/litmus/sched_gsn_edf.c	2017-07-12 22:57:41.000000000 +0200
+++ litmus-rt-mdga/litmus/sched_gsn_edf.c	2019-05-29 16:44:22.342681044 +0200
@@ -26,6 +26,8 @@
 #include <litmus/np.h>
 
 #include <litmus/bheap.h>
+#include <litmus/affinity.h>
+#include <litmus/wait.h>
 
 #ifdef CONFIG_SCHED_CPU_AFFINITY
 #include <litmus/affinity.h>
@@ -33,7 +35,7 @@
 
 /* to set up domain/cpu mappings */
 #include <litmus/litmus_proc.h>
-
+#include <linux/uaccess.h>
 #include <linux/module.h>
 
 /* Overview of GSN-EDF operations.
@@ -238,6 +240,11 @@
 	}
 }
 
+static lt_t prio_point(int eprio)
+{
+	/* make sure we have non-negative prio points */
+	return eprio + LITMUS_MAX_PRIORITY;
+}
 
 /* preempt - force a CPU to reschedule
  */
@@ -924,6 +931,254 @@
 	return &sem->litmus_lock;
 }
 
+
+/* ******************** MDGA support ********************** */
+
+struct mdga_semaphore {
+	struct litmus_lock litmus_lock;
+
+	/* current resource holder */
+	struct task_struct *owner;
+
+	/* priority queue of waiting tasks */
+	/* here ordered by dependency graph */
+	wait_queue_head_t wait;
+
+	/* current serving task */
+	atomic_t serving_ticket;
+};
+
+static inline struct mdga_semaphore* mdga_from_lock(struct litmus_lock* lock)
+{
+	return container_of(lock, struct mdga_semaphore, litmus_lock);
+}
+
+int gsnedf_mdga_lock(struct litmus_lock* l)
+{
+	struct task_struct* t = current;
+	struct mdga_semaphore *sem = mdga_from_lock(l);
+	/* priority wait queue ordered by the dependency graph */
+	prio_wait_queue_t wait;
+	unsigned long flags;
+    unsigned int current_cs;
+	unsigned int current_serving_ticket;
+    int task_order;
+    unsigned int job_number;
+    unsigned int total_jobs;
+    unsigned int total_cs;
+    int order_index;
+
+	if (!is_realtime(t))
+		return -EPERM;
+
+	/* prevent nested lock acquisition */
+	if (tsk_rt(t)->num_locks_held ||
+	    tsk_rt(t)->num_local_locks_held)
+		return -EBUSY;
+
+	preempt_disable();
+
+	spin_lock_irqsave(&sem->wait.lock, flags);
+
+    current_serving_ticket = atomic_read(&sem->serving_ticket);
+    job_number = get_jobno(t);
+    current_cs = get_current_cs(t);
+    total_jobs = get_total_jobs(t);
+    total_cs = get_cs_total(t);
+
+    order_index = (int)((job_number-3) % total_jobs);
+    task_order = get_rt_order(t)[order_index*total_cs + current_cs];
+
+    /* update the deadline */
+	tsk_rt(t)->job_params.deadline += get_rt_ddls(t)[total_jobs + order_index*(total_cs*2) + current_cs*2];
+
+	//preempt_enable_no_resched();
+	//TRACE_TASK(T, "Request a resource, current serving ticket is:%d, current job ticket is:%d", current_serving_ticket, task_order);
+	/* if the semaphore is occupied or the order is not correct */
+	if (sem->owner || current_serving_ticket != task_order) {
+		/* resource is not free => must suspend and wait */
+
+		/* ordered by the dependency order */
+		init_prio_waitqueue_entry(&wait, t, prio_point(task_order));
+
+		/* FIXME: interruptible would be nice some day */
+		set_task_state(t, TASK_UNINTERRUPTIBLE);
+
+		__add_wait_queue_prio_exclusive(&sem->wait, &wait);
+
+		TS_LOCK_SUSPEND;
+
+		/* release lock before sleeping */
+
+		spin_unlock_irqrestore(&sem->wait.lock, flags);
+
+		preempt_enable_no_resched();
+		schedule();
+
+		preempt_disable();
+		TS_LOCK_RESUME;
+
+		/* Since we hold the lock, no other task will change
+		 * ->owner. We can thus check it without acquiring the spin
+		 * lock. */
+		BUG_ON(sem->owner != t);
+	} else {
+		/* it's ours now */
+		sem->owner = t;
+		spin_unlock_irqrestore(&sem->wait.lock, flags);
+	}
+
+	tsk_rt(t)->num_locks_held++;
+
+	preempt_enable();
+	return 0;
+
+}
+
+int gsnedf_mdga_unlock(struct litmus_lock* l)
+{
+	struct task_struct *t = current;
+	struct task_struct *next = NULL;
+	struct task_struct *next_candi = NULL;
+	struct mdga_semaphore *sem = mdga_from_lock(l);
+	unsigned long flags;
+	int err = 0;
+
+	unsigned int next_serving_ticket = -1;
+	unsigned int task_order = -1;
+	unsigned int max_serving_ticket = -1;
+    unsigned int job_number;
+    unsigned int total_jobs;
+    int order_index;
+    unsigned int current_cs;
+    unsigned int total_cs;
+
+    preempt_disable();
+
+	spin_lock_irqsave(&sem->wait.lock, flags);
+
+	if (sem->owner != t) {
+		err = -EINVAL;
+		goto out;
+	}
+    job_number = get_jobno(t);
+    current_cs = get_current_cs(t);
+    total_jobs = get_total_jobs(t);
+    total_cs = get_cs_total(t);
+    order_index = (job_number-3)%total_jobs;
+    /* update the deadline */
+	tsk_rt(t)->job_params.deadline += get_rt_ddls(t)[total_jobs + order_index*(total_cs*2) + current_cs*2 + 1];
+
+	max_serving_ticket = get_rt_order(t)[total_jobs*total_cs + current_cs];
+
+	// update the current cs for the job which has finished its critical section
+	if (current_cs == total_cs-1) {
+	    tsk_rt(t)->task_params.current_cs = 0;
+	} else {
+	    tsk_rt(t)->task_params.current_cs++;
+	}
+
+	tsk_rt(t)->num_locks_held--;
+
+	/* go to next task */
+	atomic_add(1, &sem->serving_ticket);
+
+	next_serving_ticket = atomic_read(&sem->serving_ticket);
+
+	/* one graph is finished, starts next iteration */
+	if (next_serving_ticket > max_serving_ticket) {
+		next_serving_ticket = 0;
+		atomic_set(&sem->serving_ticket, 0);
+	}
+
+	/* check if there are jobs waiting for this resource */
+	next_candi = __check_waitqueue_first(&sem->wait);
+
+	if (next_candi) {
+        job_number = get_jobno(next_candi);
+        total_jobs = get_total_jobs(next_candi);
+        current_cs = get_current_cs(next_candi);
+        total_cs = get_cs_total(next_candi);
+        order_index = (int)((job_number-3)%total_jobs);
+        task_order = get_rt_order(next_candi)[order_index*total_cs + current_cs];
+	    //TRACE_TASK(next_candi, "Next candidate, next serving ticket is:%d, next candi ticket is:%d", next_serving_ticket, task_order);
+	}
+
+	if (task_order == next_serving_ticket) {
+		next = __waitqueue_remove_first(&sem->wait);
+	} else {
+		sem->owner = NULL;
+	}
+
+	if (next) {
+		/* next becomes the resouce holder */
+		sem->owner = next;
+
+		/* wake up next */
+		wake_up_process(next);
+	} else
+		/* resource becomes available */
+		sem->owner = NULL;
+
+out:
+	spin_unlock_irqrestore(&sem->wait.lock, flags);
+
+	preempt_enable();
+
+	return err;
+}
+
+/* gsnedf_mdga_open is needed or not? */
+/* transfer the dependency graph using config? */
+
+int gsnedf_mdga_close(struct litmus_lock* l)
+{
+	struct task_struct *t = current;
+	struct mdga_semaphore *sem = mdga_from_lock(l);
+	unsigned long flags;
+
+	int owner;
+
+	spin_lock_irqsave(&sem->wait.lock, flags);
+
+	owner = sem->owner == t;
+
+	spin_unlock_irqrestore(&sem->wait.lock, flags);
+
+	if (owner)
+		gsnedf_mdga_unlock(l);
+
+	return 0;
+}
+
+void gsnedf_mdga_free(struct litmus_lock* lock)
+{
+	kfree(mdga_from_lock(lock));
+}
+
+static struct litmus_lock_ops gsnedf_mdga_lock_ops = {
+	.close  = gsnedf_mdga_close,
+	.lock   = gsnedf_mdga_lock,
+	.unlock = gsnedf_mdga_unlock,
+	.deallocate = gsnedf_mdga_free,
+};
+
+static struct litmus_lock* gsnedf_new_mdga(void)
+{
+	struct mdga_semaphore* sem;
+
+	sem = kmalloc(sizeof(*sem), GFP_KERNEL);
+	if (!sem)
+		return NULL;
+
+	sem->owner   = NULL;
+	init_waitqueue_head(&sem->wait);
+	sem->litmus_lock.ops = &gsnedf_mdga_lock_ops;
+	atomic_set(&sem->serving_ticket, 0);
+
+	return &sem->litmus_lock;
+}
+
 /* **** lock constructor **** */
 
 
@@ -941,6 +1196,15 @@
 		if (*lock)
 			err = 0;
 		else
+			err = -ENOMEM;
+		break;
+
+	case MDGA_SEM:
+		/* Multi-Critical Sections Dependency Graph Approach */
+		*lock = gsnedf_new_mdga();
+		if (*lock)
+			err = 0;
+		else
 			err = -ENOMEM;
 		break;
 
diff -Naur litmus-rt-original/litmus/sched_psn_edf.c litmus-rt-mdga/litmus/sched_psn_edf.c
--- litmus-rt-original/litmus/sched_psn_edf.c	2017-07-12 22:57:41.000000000 +0200
+++ litmus-rt-mdga/litmus/sched_psn_edf.c	2019-05-29 16:45:03.622726282 +0200
@@ -24,9 +24,11 @@
 #include <litmus/edf_common.h>
 #include <litmus/sched_trace.h>
 #include <litmus/trace.h>
+#include <litmus/wait.h>
 
 /* to set up domain/cpu mappings */
 #include <litmus/litmus_proc.h>
+#include <linux/uaccess.h>
 
 typedef struct {
 	rt_domain_t 		domain;
@@ -146,6 +148,12 @@
 		return 0;
 }
 
+static lt_t prio_point(int eprio)
+{
+	/* make sure we have non-negative prio points */
+	return eprio + LITMUS_MAX_PRIORITY;
+}
+
 /* This check is trivial in partioned systems as we only have to consider
  * the CPU of the partition.
  */
@@ -537,6 +545,264 @@
 	return &sem->litmus_lock;
 }
 
+/* ******************** MDGA support ********************** */
+struct mdga_semaphore {
+	struct litmus_lock litmus_lock;
+
+	/* current resource holder */
+	struct task_struct *owner;
+
+	/* priority queue of waiting tasks */
+	/* here ordered by dependency graph */
+	wait_queue_head_t wait;
+
+	/* ceiling priority is not needed */
+
+	/* current serving task */
+	atomic_t serving_ticket;
+};
+
+static inline struct mdga_semaphore* mdga_from_lock(struct litmus_lock* lock)
+{
+	return container_of(lock, struct mdga_semaphore, litmus_lock);
+}
+
+int psnedf_mdga_lock(struct litmus_lock* l)
+{
+	struct task_struct* t = current;
+	struct mdga_semaphore *sem = mdga_from_lock(l);
+	/* priority wait queue ordered by the dependency graph */
+	prio_wait_queue_t wait;
+	unsigned long flags;
+
+	unsigned int current_cs;
+	unsigned int current_serving_ticket;
+    int task_order;
+    unsigned int job_number;
+    unsigned int total_jobs;
+    unsigned int total_cs;
+    int order_index;
+
+	if (!is_realtime(t))
+		return -EPERM;
+
+	/* prevent nested lock acquisition */
+	if (tsk_rt(t)->num_locks_held ||
+	    tsk_rt(t)->num_local_locks_held)
+		return -EBUSY;
+
+	preempt_disable();
+
+	spin_lock_irqsave(&sem->wait.lock, flags);
+
+    current_serving_ticket = atomic_read(&sem->serving_ticket);
+
+    job_number = get_jobno(t);
+    current_cs = get_current_cs(t);
+    total_jobs = get_total_jobs(t);
+    total_cs = get_cs_total(t);
+
+    order_index = (int)((job_number-3) % total_jobs);
+    task_order = get_rt_order(t)[order_index*total_cs + current_cs];
+
+    /* update the deadline */
+	tsk_rt(t)->job_params.deadline += get_rt_ddls(t)[total_jobs + order_index*(total_cs*2) + current_cs*2 + 1];
+
+	/* if the semaphore is occupied or the order is not correct */
+	if (sem->owner || current_serving_ticket != task_order) {
+		/* resource is not free => must suspend and wait */
+
+		/* ordered by the dependency order */
+		init_prio_waitqueue_entry(&wait, t, prio_point(task_order));
+
+		/* FIXME: interruptible would be nice some day */
+		set_task_state(t, TASK_UNINTERRUPTIBLE);
+
+		__add_wait_queue_prio_exclusive(&sem->wait, &wait);
+
+		TS_LOCK_SUSPEND;
+
+		/* release lock before sleeping */
+		preempt_enable_no_resched();
+		spin_unlock_irqrestore(&sem->wait.lock, flags);
+
+		schedule();
+
+		TS_LOCK_RESUME;
+
+		/* Since we hold the lock, no other task will change
+		 * ->owner. We can thus check it without acquiring the spin
+		 * lock. */
+		BUG_ON(sem->owner != t);
+	} else {
+		/* it's ours now */
+		sem->owner = t;
+
+		/* mark the task as priority-boosted. */
+		boost_priority(t);
+        preempt_enable_no_resched();
+		spin_unlock_irqrestore(&sem->wait.lock, flags);
+	}
+
+	tsk_rt(t)->num_locks_held++;
+
+	return 0;
+
+}
+
+int psnedf_mdga_unlock(struct litmus_lock* l)
+{
+	struct task_struct *t = current;
+	struct task_struct *next = NULL;
+	struct task_struct *next_candi = NULL;
+	struct mdga_semaphore *sem = mdga_from_lock(l);
+	unsigned long flags;
+	int err = 0;
+
+	unsigned int next_serving_ticket = -1;
+	unsigned int task_order = -1;
+	unsigned int max_serving_ticket = -1;
+    unsigned int job_number;
+    unsigned int total_jobs;
+    int order_index;
+    unsigned int current_cs;
+    unsigned int total_cs;
+
+    preempt_disable();
+
+	spin_lock_irqsave(&sem->wait.lock, flags);
+
+	if (sem->owner != t) {
+		err = -EINVAL;
+		goto out;
+	}
+
+	job_number = get_jobno(t);
+    current_cs = get_current_cs(t);
+    total_jobs = get_total_jobs(t);
+    total_cs = get_cs_total(t);
+    order_index = (job_number-3)%total_jobs;
+
+    /* update the deadline */
+	tsk_rt(t)->job_params.deadline += get_rt_ddls(t)[total_jobs + order_index*(total_cs*2) + current_cs*2 + 1];
+
+	max_serving_ticket = get_rt_order(t)[total_jobs*total_cs + current_cs];
+
+	// update the current cs for the job which has finished its critical section
+	if (current_cs == total_cs-1) {
+	    tsk_rt(t)->task_params.current_cs = 0;
+	} else {
+	    tsk_rt(t)->task_params.current_cs++;
+	}
+
+	tsk_rt(t)->num_locks_held--;
+
+	/* go to next task */
+	atomic_add(1, &sem->serving_ticket);
+
+	next_serving_ticket = atomic_read(&sem->serving_ticket);
+
+	/* one graph is finished, starts next iteration */
+	if (next_serving_ticket > max_serving_ticket) {
+		next_serving_ticket = 0;
+		atomic_set(&sem->serving_ticket, 0);
+	}
+
+	/* we lose the benefit of priority boosting */
+
+	unboost_priority(t);
+
+	/* check if there are jobs waiting for this resource */
+	next_candi = __check_waitqueue_first(&sem->wait);
+
+	if (next_candi) {
+        job_number = get_jobno(next_candi);
+        total_jobs = get_total_jobs(next_candi);
+        current_cs = get_current_cs(next_candi);
+        total_cs = get_cs_total(next_candi);
+        order_index = (int)((job_number-3)%total_jobs);
+        task_order = get_rt_order(next_candi)[order_index*total_cs + current_cs];
+	}
+
+	if (task_order == next_serving_ticket) {
+		next = __waitqueue_remove_first(&sem->wait);
+	} else {
+		sem->owner = NULL;
+	}
+
+	if (next) {
+		/* boost next job */
+		boost_priority(next);
+
+		/* next becomes the resouce holder */
+		sem->owner = next;
+
+		/* wake up next */
+		wake_up_process(next);
+	} else
+		/* resource becomes available */
+		sem->owner = NULL;
+
+out:
+	spin_unlock_irqrestore(&sem->wait.lock, flags);
+
+	preempt_enable();
+
+	return err;
+}
+
+/* psnedf_mdga_open is needed or not? */
+/* transfer the dependency graph using config? */
+
+int psnedf_mdga_close(struct litmus_lock* l)
+{
+	struct task_struct *t = current;
+	struct mdga_semaphore *sem = mdga_from_lock(l);
+	unsigned long flags;
+
+	int owner;
+
+	spin_lock_irqsave(&sem->wait.lock, flags);
+
+	owner = sem->owner == t;
+
+	spin_unlock_irqrestore(&sem->wait.lock, flags);
+
+	if (owner)
+		psnedf_mdga_unlock(l);
+
+	return 0;
+}
+
+void psnedf_mdga_free(struct litmus_lock* lock)
+{
+	kfree(mdga_from_lock(lock));
+}
+
+static struct litmus_lock_ops psnedf_mdga_lock_ops = {
+	.close  = psnedf_mdga_close,
+	.lock   = psnedf_mdga_lock,
+	.unlock = psnedf_mdga_unlock,
+	.deallocate = psnedf_mdga_free,
+};
+
+static struct litmus_lock* psnedf_new_mdga(void)
+{
+	struct mdga_semaphore* sem;
+
+	sem = kmalloc(sizeof(*sem), GFP_KERNEL);
+	if (!sem)
+		return NULL;
+
+	sem->owner   = NULL;
+	init_waitqueue_head(&sem->wait);
+	sem->litmus_lock.ops = &psnedf_mdga_lock_ops;
+	atomic_set(&sem->serving_ticket, 0);
+
+	return &sem->litmus_lock;
+}
+
+
 /* **** lock constructor **** */
 
 
@@ -555,6 +821,14 @@
 		if (*lock)
 			err = 0;
 		else
+			err = -ENOMEM;
+		break;
+    case MDGA_SEM:
+		/* Harmonic Dependency Graph Approach */
+		*lock = psnedf_new_mdga();
+		if (*lock)
+			err = 0;
+		else
 			err = -ENOMEM;
 		break;
 
